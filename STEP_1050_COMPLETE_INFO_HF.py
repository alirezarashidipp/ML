import json
import os

# ---------------------------------------------------------
# 1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ (CONFIGURATION) - Ú©Ù†ØªØ±Ù„ Ù¾Ù†Ù„ Ø´Ù…Ø§
# ---------------------------------------------------------
CONFIG = {
    # Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆØ¯: 'HF' (Ù‡Ø§Ù„ÛŒÙ†Ú¯ ÙÛŒØ³), 'LOCAL' (Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡), 'HSBC_API' (Ø³Ø±ÙˆØ± Ø´Ø±Ú©Øª)
    "SOURCE_MODE": "HF", 
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Hugging Face
    "HF_MODEL_ID": "meta-llama/Llama-3.2-1B-Instruct",
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Local (Ø¢Ø¯Ø±Ø³ Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø³ÛŒØ³ØªÙ… Ø´Ù…Ø§)
    "LOCAL_PATH": "/content/drive/MyDrive/models/llama-1b-v2", 
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª API Ø´Ø±Ú©Øª (HSBC)
    "API_URL": "https://api.hsbc.internal/v1/chat/completions",
    "API_KEY": "sk-xxxxxxxxxxxxxxxxxxxxxxxx",
    "API_TIMEOUT": 30
}

# ---------------------------------------------------------
# 2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù„ÙˆÚ©Ø§Ù„ Ùˆ HF)
# ---------------------------------------------------------
pipeline_instance = None

if CONFIG["SOURCE_MODE"] in ["HF", "LOCAL"]:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

    # ØªØ¹ÛŒÛŒÙ† Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ù…Ø§
    model_path = CONFIG["HF_MODEL_ID"] if CONFIG["SOURCE_MODE"] == "HF" else CONFIG["LOCAL_PATH"]
    
    print(f"ğŸ”„ Initializing Model from source: {CONFIG['SOURCE_MODE']} ({model_path})...")
    
    try:
        # ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto"
        )
        
        pipeline_instance = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=500,
            temperature=0.1,
            do_sample=True
        )
        print("âœ… Model loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        print("Hint: If using LOCAL, make sure the path is correct.")

elif CONFIG["SOURCE_MODE"] == "HSBC_API":
    import requests
    print("âœ… System configured for API usage. No local model loading needed.")

# ---------------------------------------------------------
# 3. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ (Abstraction Layer)
# ---------------------------------------------------------

def get_llm_response(messages):
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªØµÙ…ÛŒÙ… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ù‡ Ú©Ø¬Ø§ Ø¨ÙØ±Ø³ØªØ¯
    Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙ†Ø¸ÛŒÙ…Ø§Øª CONFIG["SOURCE_MODE"]
    """
    mode = CONFIG["SOURCE_MODE"]
    
    # --- Ø±ÙˆØ´ Û± Ùˆ Û²: Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø®ÙˆØ¯Ù…Ø§Ù† (HF / LOCAL) ---
    if mode in ["HF", "LOCAL"]:
        if pipeline_instance is None:
            return "Error: Model not loaded."
        
        outputs = pipeline_instance(messages)
        return outputs[0]["generated_text"][-1]["content"]

    # --- Ø±ÙˆØ´ Û³: ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API Ø´Ø±Ú©Øª (HSBC) ---
    elif mode == "HSBC_API":
        # Ø§Ú©Ø«Ø± APIÙ‡Ø§ÛŒ Ø´Ø±Ú©ØªÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ OpenAI ÛŒØ§ Azure Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
        payload = {
            "model": "gpt-4-turbo-internal", # ÛŒØ§ Ù†Ø§Ù… Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø´Ø±Ú©Øª Ø¯Ø§Ø¯Ù‡
            "messages": messages,
            "temperature": 0.1,
            "max_tokens": 500
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['API_KEY']}", 
            # Ú¯Ø§Ù‡ÛŒ Ø¨Ø§Ù†Ú©â€ŒÙ‡Ø§ Ù‡Ø¯Ø±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯ Ù…Ø«Ù„:
            # "x-api-key": CONFIG['API_KEY'],
            # "Ocp-Apim-Subscription-Key": CONFIG['API_KEY'] (Ø§Ú¯Ø± Azure Ø¨Ø§Ø´Ø¯)
        }
        
        try:
            response = requests.post(
                CONFIG["API_URL"], 
                json=payload, 
                headers=headers, 
                timeout=CONFIG["API_TIMEOUT"],
                verify=False # Ø¯Ø± Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù†Ú©ÛŒ Ú¯Ø§Ù‡ÛŒ SSL Ø¯Ø§Ø®Ù„ÛŒ self-signed Ø§Ø³Øª
            )
            response.raise_for_status()
            
            # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ø¬ÙˆØ§Ø¨ (Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø³Ø§Ø®ØªØ§Ø± choices[0].message.content Ø¯Ø§Ø±Ù†Ø¯)
            data = response.json()
            return data["choices"][0]["message"]["content"]
            
        except Exception as e:
            return f"API Error: {str(e)}"

def clean_json_output(text):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON"""
    text = text.strip()
    if text.startswith("API Error") or text.startswith("Error"):
        return None
        
    if text.startswith("```json"):
        text = text.replace("```json", "", 1)
    if text.startswith("```"):
        text = text.replace("```", "", 1)
    if text.endswith("```"):
        text = text[:-3]
    return text.strip()

# ---------------------------------------------------------
# 4. Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ (Main Logic - Ø«Ø§Ø¨Øª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§)
# ---------------------------------------------------------

def analyze_jira_ticket(ticket_text):
    system_prompt = """You are a JIRA analysis engine.
    Allowed Intents: [Create, Modify, Remove, Migrate, Integrate, Investigate, Enforce].
    
    Return a VALID JSON object with this structure:
    {
      "story_ownership": { "identified": boolean, "confidence": int, "owner": string or null },
      "primary_intent": { "defined": boolean, "confidence": int, "type": string }
    }
    Output ONLY JSON.
    """
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": ticket_text},
    ]
    
    print(f"\nğŸš€ Processing via [{CONFIG['SOURCE_MODE']}]...")
    
    # 1. Ú¯Ø±ÙØªÙ† Ù…ØªÙ† Ø®Ø§Ù… Ø§Ø² Ù‡Ø± Ù…Ù†Ø¨Ø¹ÛŒ Ú©Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
    raw_response = get_llm_response(messages)
    
    # 2. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¢Ø¨Ø¬Ú©Øª Ùˆ Ù†Ù…Ø§ÛŒØ´
    clean_text = clean_json_output(raw_response)
    
    if clean_text is None:
        print("âŒ Failed to get valid response.")
        print("Raw:", raw_response)
        return

    try:
        result = json.loads(clean_text)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ø±ÙˆØ¬ÛŒ
        print("-" * 30)
        so = result.get('story_ownership', {})
        print(f"STORY OWNERSHIP")
        print(f"Identified:      {'Yes' if so.get('identified') else 'No'} ({so.get('confidence', 0)}%)")
        print(f"Extracted Owner: {so.get('owner', 'N/A')}")
        
        print("-" * 30)
        pi = result.get('primary_intent', {})
        print(f"PRIMARY INTENT")
        print(f"Clearly Defined: {'Yes' if pi.get('defined') else 'No'} ({pi.get('confidence', 0)}%)")
        print(f"Intent Type:     {pi.get('type', 'N/A')}")
        print("-" * 30)
        
    except json.JSONDecodeError:
        print("âŒ JSON Parsing Error. Model output was not valid JSON.")
        print("Raw Output:", raw_response)

# ---------------------------------------------------------
# 5. Ø§Ø¬Ø±Ø§
# ---------------------------------------------------------

sample_text = "I am Payment Platform Product Owner, i want to build a system that easy integrate two code bases toghether."

analyze_jira_ticket(sample_text)
